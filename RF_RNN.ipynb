{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffc617e2",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) for Radio Frequency (RF) Signal Analysis\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks designed to process sequential data. Unlike feedforward networks, RNNs maintain a hidden state that carries information across time steps, making them suitable for time-series tasks.\n",
    "\n",
    "In RF signal processing, signals are naturally sequential (time-domain waveforms), which makes RNNs particularly useful for tasks such as:\n",
    "\n",
    "- Modulation classification\n",
    "- Signal detection\n",
    "- Spectrum monitoring\n",
    "- Anomaly detection\n",
    "\n",
    "---\n",
    "\n",
    "## 2. How RNNs Work\n",
    "\n",
    "- **Input:** Sequence of feature vectors \\( x_1, x_2, ..., x_T \\) over time steps \\( T \\).  \n",
    "- **Hidden State:** \\( h_t = f(W_{xh} x_t + W_{hh} h_{t-1} + b_h) \\)  \n",
    "  The hidden state captures the memory of previous inputs.  \n",
    "- **Output:** \\( y_t = g(W_{hy} h_t + b_y) \\)  \n",
    "  Depending on the task, the network can produce a prediction at each time step or only at the final step.\n",
    "\n",
    "### Limitations of Vanilla RNNs\n",
    "\n",
    "- Suffer from vanishing/exploding gradients.\n",
    "- Struggle to learn long-term dependencies.\n",
    "- Typically replaced by **GRUs** or **LSTMs** for longer sequences.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. GRU and LSTM\n",
    "\n",
    "**GRU (Gated Recurrent Unit):**\n",
    "- Uses reset and update gates to control memory.\n",
    "- Fewer parameters than LSTM, faster to train.\n",
    "- Handles moderate long-term dependencies.\n",
    "\n",
    "**LSTM (Long Short-Term Memory):**\n",
    "- Uses cell state with input, forget, and output gates.\n",
    "- Better at learning long-term dependencies.\n",
    "- Slightly more computationally expensive than GRU.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Applying RNNs to RF Signals\n",
    "\n",
    "RF signals are sequences of samples (e.g., amplitude, phase, or I/Q components). RNNs can model temporal dependencies in these sequences for various tasks:\n",
    "\n",
    "### Example Applications\n",
    "\n",
    "1. **Modulation Classification**\n",
    "   - Input: Sequence of I/Q samples or amplitude values.\n",
    "   - Output: Predicted modulation scheme (e.g., BPSK, QPSK, QAM).\n",
    "   - RNN can learn the temporal patterns characteristic of each modulation.\n",
    "\n",
    "2. **Signal Detection**\n",
    "   - Detect presence of a specific signal within a noisy channel.\n",
    "   - Output: Probability of signal presence per time step or sequence.\n",
    "\n",
    "3. **Anomaly Detection**\n",
    "   - Identify unusual RF patterns (e.g., jamming, interference).\n",
    "   - RNNs learn normal temporal patterns; deviations are flagged.\n",
    "\n",
    "### Data Representation\n",
    "\n",
    "| Signal Type | Input Format |\n",
    "|-------------|-------------|\n",
    "| BPSK        | 1D sequence of real amplitudes (+1/-1) |\n",
    "| QPSK        | 2D sequence of I/Q values |\n",
    "| QAM         | Multi-dimensional sequence of constellation components |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Advantages of RNNs for RF Analysis\n",
    "\n",
    "- Capture temporal dependencies in signals.\n",
    "- Can handle sequences of variable length.\n",
    "- Learn features automatically, reducing need for manual feature engineering.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Practical Considerations\n",
    "\n",
    "- **Sequence Length:** Must match sampling interval and duration of RF snippet.\n",
    "- **Batching:** Use DataLoader with padding or fixed-length sequences.\n",
    "- **Model Choice:** Use LSTM/GRU for longer sequences; vanilla RNN suffices for short sequences.\n",
    "- **GPU Acceleration:** Recommended for large datasets or long sequences.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. References\n",
    "\n",
    "1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.\n",
    "2. O’Shea, T., & Hoydis, J. (2017). *An Introduction to Deep Learning for the Physical Layer*. IEEE Transactions on Cognitive Communications and Networking.\n",
    "3. Hochreiter, S., & Schmidhuber, J. (1997). *Long Short-Term Memory*. Neural Computation, 9(8), 1735–1780.\n",
    "4. Cho, K. et al. (2014). *Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation*. EMNLP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeab08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Synthetic RF Dataset\n",
    "# --------------------\n",
    "# The RFDataset class generates toy radio-frequency signals.\n",
    "# It supports BPSK (1D real-valued samples) and QPSK (2D I/Q samples).\n",
    "# Each dataset instance returns (signal, label).\n",
    "class RFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Torch Dataset for RF signals.\n",
    "    Generates toy signals with either BPSK or QPSK modulation.\n",
    "    Each sample is (signal, label).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples=1000, seq_len=128):\n",
    "        half = num_samples // 2\n",
    "        bpsk = self.generate_rf_data(half, seq_len, \"BPSK\")\n",
    "        qpsk = self.generate_rf_data(half, seq_len, \"QPSK\")\n",
    "\n",
    "        X = np.concatenate([bpsk, qpsk], axis=0)\n",
    "        y = np.array([0] * half + [1] * half)\n",
    "\n",
    "        # Shuffle dataset to mix classes\n",
    "        idx = np.random.permutation(num_samples)\n",
    "        self.X, self.y = X[idx], y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_rf_data(num_samples, seq_len, modulation):\n",
    "        \"\"\"\n",
    "        Generate synthetic RF signals.\n",
    "        - BPSK: +1/-1 values (real).\n",
    "        - QPSK: maps bit pairs to complex constellation points (I/Q).\n",
    "        \"\"\"\n",
    "        if modulation == \"BPSK\":\n",
    "            symbols = np.random.choice([1, -1], size=(num_samples, seq_len))\n",
    "            return symbols.astype(np.float32)[..., None]  # shape [N, L, 1]\n",
    "        elif modulation == \"QPSK\":\n",
    "            bits = np.random.choice([0, 1], size=(num_samples, seq_len, 2))\n",
    "            mapping = {(0, 0): 1+1j, (0, 1): -1+1j, (1, 0): 1-1j, (1, 1): -1-1j}\n",
    "            symbols = np.array([[mapping[tuple(b)] for b in row] for row in bits])\n",
    "            return np.stack([symbols.real, symbols.imag], axis=-1).astype(np.float32)  # [N, L, 2]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported modulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea437cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# RNN Model\n",
    "# --------------------\n",
    "# RFSignalRNN is a simple recurrent neural network for sequence classification.\n",
    "# It reads input sequences of RF samples and predicts the modulation scheme.\n",
    "class RFSignalRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN classifier for RF signals.\n",
    "    Takes sequence input [batch, seq_len, input_dim] and predicts modulation class.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1, hidden_dim=64, num_layers=1, num_classes=2):\n",
    "        super(RFSignalRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_dim,\n",
    "                          hidden_size=hidden_dim,\n",
    "                          num_layers=num_layers,\n",
    "                          batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)       # [batch, seq_len, hidden_dim]\n",
    "        out = out[:, -1, :]        # last timestep output\n",
    "        return self.fc(out)        # [batch, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb9c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Training Routine\n",
    "# --------------------\n",
    "# train_model handles dataset preparation, batching, GPU usage, and training loop.\n",
    "# It reports training loss and test accuracy for each epoch.\n",
    "def train_model(device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    \"\"\"\n",
    "    Train RNN on synthetic RF dataset with GPU acceleration.\n",
    "    Uses DataLoader for batching and shuffling.\n",
    "    \"\"\"\n",
    "    # Dataset + loaders\n",
    "    train_ds = RFDataset(num_samples=4000, seq_len=128)\n",
    "    test_ds = RFDataset(num_samples=1000, seq_len=128)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Model\n",
    "    input_dim = train_ds.X.shape[-1]\n",
    "    model = RFSignalRNN(input_dim=input_dim, hidden_dim=64, num_layers=1, num_classes=2).to(device)\n",
    "\n",
    "    # Loss + optimiser\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * X.size(0)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                preds = model(X).argmax(dim=1)\n",
    "                correct += (preds == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "        acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={total_loss/len(train_ds):.4f}, Test Acc={acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Script Entry Point\n",
    "# --------------------\n",
    "# The script starts here: runs the training routine.\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
